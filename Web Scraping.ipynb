{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329a4194-62ce-4584-9b46-ee8641c1cb06",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. It involves fetching a webpage, parsing its content, and collecting specific information such as text, images, or other structured data.\n",
    "\n",
    "\n",
    "\n",
    "## How Does Web Scraping Work?\n",
    "\n",
    "**Send a Request:**\n",
    "Use an HTTP library (e.g., requests) to send a request to a webpage's URL.\n",
    "The server responds with the webpage's HTML content.\n",
    "\n",
    "**Parse the Response:**\n",
    "Use an HTML parsing library (e.g., BeautifulSoup) to extract specific elements (like text, links, tables).\n",
    "\n",
    "**Extract Data:**\n",
    "Target specific HTML elements (e.g., tags, classes, or IDs) to collect the desired information.\n",
    "\n",
    "**Save the Data:**\n",
    "Store the scraped data in a structured format such as a CSV file, database, or JSON.\n",
    "\n",
    "\n",
    "## Why use Web Scraping?\n",
    "\n",
    "\n",
    "Web scraping is a powerful technique used to extract data from websites in an automated and efficient manner. It involves fetching a webpage’s HTML content, parsing it, and extracting specific information such as text, images, or structured data like tables. This process is widely used because websites often contain vast amounts of valuable data that are not readily available in downloadable formats like APIs or public datasets. Web scraping is particularly advantageous when large-scale data collection is required for business insights, competitive analysis, or academic research.\n",
    "\n",
    "Businesses use web scraping to monitor competitors' pricing, product availability, and customer reviews, enabling them to make informed decisions and stay ahead in the market. For instance, an e-commerce retailer might scrape product prices from competitors to dynamically adjust their own pricing. Researchers often leverage web scraping to gather datasets from news websites, social media platforms, or public resources to analyze trends and behavior. It is also extensively used in industries like real estate to scrape property listings, finance to collect stock market data, and travel to compare flight or hotel prices.\n",
    "\n",
    "The process of web scraping typically involves sending a request to a webpage using an HTTP client, parsing the returned HTML using libraries like BeautifulSoup or lxml, and then extracting the desired data based on tags, attributes, or class names. Tools like Selenium or Playwright are often used for dynamic content that is generated by JavaScript. Once collected, the data is cleaned and saved in structured formats such as CSV, JSON, or databases for further analysis. Web scraping is also a key enabler for machine learning applications, as it allows the collection of large datasets required to train models for tasks like sentiment analysis or recommendation systems.\n",
    "\n",
    "One of the main reasons web scraping is preferred is its ability to automate repetitive tasks, such as monitoring stock prices or gathering daily job postings. It also helps fill gaps where APIs are unavailable or limited in functionality. For example, many websites do not offer APIs for public data access, so web scraping becomes the only option to collect this information. Another benefit is the scalability of web scraping, as it can handle large amounts of data across multiple websites, saving time and effort compared to manual collection.\n",
    "\n",
    "Despite its advantages, web scraping has certain challenges. Dynamic websites that load content through JavaScript can make data extraction more complex, requiring advanced tools like Selenium or Puppeteer. Additionally, many websites employ anti-scraping mechanisms, such as CAPTCHAs or rate-limiting, to detect and block bots. Legal and ethical considerations also play a significant role, as scraping some websites may violate their terms of service. Therefore, it’s essential to review the website’s `robots.txt` file or use available APIs wherever possible to stay compliant.\n",
    "\n",
    "In summary, web scraping is an essential tool for gathering large-scale data efficiently, driving business intelligence, and automating data collection processes. It empowers industries, researchers, and developers to analyze trends, optimize strategies, and build advanced applications. However, it should always be done responsibly, adhering to legal and ethical guidelines.\n",
    "\n",
    "\n",
    "## What industries benefit most from scraping?\n",
    "Industries like e-commerce, travel, real estate, finance, and marketing benefit the most from web scraping due to their reliance on vast and dynamic datasets. It enables these industries to automate data collection, improve decision-making, and stay competitive in a fast-changing market. However, it’s essential to perform web scraping ethically, respecting legal boundaries and the website’s terms of service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff01a79-7fae-455b-a4c7-49e3ed5ea725",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Best Tools for Web Scraping\n",
    "\n",
    "Web scraping tools can vary based on complexity, use case, and technical requirements. Here’s a breakdown of the best tools for web scraping:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Python Libraries**\n",
    "\n",
    "### **a) `BeautifulSoup`**\n",
    "- **Best For**: Beginners and small-scale scraping of static websites.\n",
    "- **Description**: A simple library for parsing HTML and XML. Allows easy navigation of HTML elements.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    url = \"https://example.com\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract data\n",
    "    titles = soup.find_all(\"h1\")\n",
    "    for title in titles:\n",
    "        print(title.text)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **b) `Requests`**\n",
    "- **Best For**: Fetching HTML content from static websites.\n",
    "- **Description**: A library for making HTTP requests (GET, POST, etc.). Often used with `BeautifulSoup`.\n",
    "\n",
    "---\n",
    "\n",
    "### **c) `Scrapy`**\n",
    "- **Best For**: Large-scale, customizable scraping projects.\n",
    "- **Description**: A powerful framework for crawling and scraping websites. Handles requests, parsing, and data pipelines.\n",
    "- **Example**:\n",
    "    ```bash\n",
    "    scrapy startproject myproject\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **d) `Selenium`**\n",
    "- **Best For**: Scraping dynamic websites that load content via JavaScript.\n",
    "- **Description**: A browser automation tool that simulates user interaction with webpages.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://example.com\")\n",
    "    print(driver.page_source)\n",
    "    driver.quit()\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **e) `Playwright`**\n",
    "- **Best For**: Advanced, modern scraping of dynamic websites.\n",
    "- **Description**: A faster, more reliable alternative to `Selenium` for handling JavaScript-heavy pages.\n",
    "\n",
    "---\n",
    "\n",
    "### **f) `Puppeteer`**\n",
    "- **Best For**: Headless browser scraping.\n",
    "- **Description**: A Node.js library to control a Chromium browser. Great for dynamic web content.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. No-Code Scraping Tools**\n",
    "\n",
    "### **a) Octoparse**\n",
    "- **Best For**: Visual, point-and-click scraping.\n",
    "- **Description**: A user-friendly tool with a drag-and-drop interface. Supports dynamic and JavaScript-heavy websites.\n",
    "\n",
    "---\n",
    "\n",
    "### **b) ParseHub**\n",
    "- **Best For**: Scraping complex websites without code.\n",
    "- **Description**: A visual scraper that uses machine learning for intelligent data extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### **c) DataMiner**\n",
    "- **Best For**: Browser-based scraping.\n",
    "- **Description**: A Chrome/Edge browser extension for scraping data directly from webpages.\n",
    "\n",
    "---\n",
    "\n",
    "### **d) WebHarvy**\n",
    "- **Best For**: Easy scraping for small to medium-scale projects.\n",
    "- **Description**: A visual scraping tool that extracts data based on patterns defined by the user.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Advanced Tools and Frameworks**\n",
    "\n",
    "### **a) `Scrapy`**\n",
    "- **Best For**: Enterprise-grade scraping.\n",
    "- **Features**:\n",
    "  - Asynchronous requests for faster scraping.\n",
    "  - Built-in data pipelines for storing data.\n",
    "  - Extensible with middlewares for anti-scraping measures.\n",
    "\n",
    "---\n",
    "\n",
    "### **b) `Crawlera` (Smart Proxy Manager by ScrapingBee)**\n",
    "- **Best For**: Avoiding IP blocks and anti-bot mechanisms.\n",
    "- **Description**: A proxy service that rotates IPs and manages request throttling.\n",
    "\n",
    "---\n",
    "\n",
    "### **c) `ScraperAPI`**\n",
    "- **Best For**: Simplified scraping with proxy management.\n",
    "- **Description**: A service to handle rotating proxies, CAPTCHAs, and JavaScript rendering.\n",
    "\n",
    "---\n",
    "\n",
    "### **d) Apify**\n",
    "- **Best For**: Cloud-based scraping and automation.\n",
    "- **Description**: A platform for running scraping scripts in the cloud. Integrates well with popular frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Headless Browsers**\n",
    "\n",
    "### **a) Puppeteer**\n",
    "- **Best For**: Controlling Chrome/Chromium in headless mode.\n",
    "\n",
    "---\n",
    "\n",
    "### **b) Playwright**\n",
    "- **Best For**: Cross-browser automation and scraping.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. API-Based Scraping**\n",
    "\n",
    "Many websites offer APIs to access structured data, which is faster and more reliable than scraping:\n",
    "- **Twitter API**: Scraping tweets and user data.\n",
    "- **OpenWeather API**: Accessing weather data.\n",
    "- **Google Maps API**: For geospatial and location-based services.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Proxy Services**\n",
    "\n",
    "To avoid IP bans and bypass rate limits:\n",
    "- **SmartProxy**: Reliable rotating proxies.\n",
    "- **Bright Data (Luminati)**: Offers advanced scraping tools and proxy pools.\n",
    "- **Proxymesh**: Simple proxy service for bypassing geo-restrictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Cloud-Based Scraping Platforms**\n",
    "\n",
    "For large-scale scraping projects:\n",
    "- **ScrapingBee**: Handles headless browsers, proxies, and CAPTCHAs.\n",
    "- **Zyte (formerly ScrapingHub)**: Offers scraping frameworks and proxy management tools.\n",
    "- **Import.io**: Converts webpages into structured data without coding.\n",
    "\n",
    "---\n",
    "\n",
    "## **Best Tools Based on Use Cases**\n",
    "\n",
    "| **Use Case**                     | **Best Tools**                             |\n",
    "|-----------------------------------|--------------------------------------------|\n",
    "| Small-scale static scraping       | `BeautifulSoup`, `Requests`                |\n",
    "| Dynamic website scraping          | `Selenium`, `Playwright`, `Puppeteer`      |\n",
    "| Large-scale scraping              | `Scrapy`, `Apify`, `Crawlera`              |\n",
    "| No-code scraping                  | `Octoparse`, `ParseHub`, `DataMiner`       |\n",
    "| Avoiding IP bans                  | `ScraperAPI`, `SmartProxy`, `Bright Data`  |\n",
    "| API-based scraping                | APIs (e.g., Twitter API, Google Maps API)  |\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "The best tools for web scraping depend on your requirements:\n",
    "- For small projects, libraries like `BeautifulSoup` and `Requests` are simple and effective.\n",
    "- For dynamic content, tools like `Selenium`, `Playwright`, or `Puppeteer` work best.\n",
    "- For large-scale projects, frameworks like `Scrapy` or platforms like `Apify` ensure scalability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01092f0a-f197-4cef-b9e5-356889c9fd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Rotating User-Agent headers\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.amazon.in/s?bbn=81107433031&rh=n%3A81107433031%2Cp_85%3A10440599031&_encoding=UTF8&content-id=amzn1.sym.58c90a12-100b-4a2f-8e15-7c06f1abe2be&pd_rd_r=eb705f4e-d34b-456d-a496-b52f6602d46b&pd_rd_w=hwFSy&pd_rd_wg=MVPlH&pf_rd_p=58c90a12-100b-4a2f-8e15-7c06f1abe2be&pf_rd_r=DHWY31K2T4Q1ARX3NX8Z&ref=pd_hp_d_atf_unk\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Initializing lists for product data\n",
    "    product_names = []\n",
    "    product_prices = []\n",
    "    product_ratings = []\n",
    "\n",
    "    # Locating the main slot containing all products\n",
    "    product_container = soup.find(\"div\", {\"class\": \"s-main-slot s-result-list s-search-results sg-row\"})\n",
    "\n",
    "    # Iterating over individual products\n",
    "    if product_container:\n",
    "        for product in product_container.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "            # Extracting product name\n",
    "            name = product.find(\"span\", {\"class\": \"a-size-base-plus a-color-base a-text-normal\"})\n",
    "            # Extracting product price\n",
    "            price = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "            # Extracting product rating\n",
    "            rating = product.find(\"span\", {\"class\": \"a-icon-alt\"})\n",
    "\n",
    "            product_names.append(name.text.strip() if name else \"N/A\")\n",
    "            product_prices.append(price.text.strip() if price else \"N/A\")\n",
    "            product_ratings.append(rating.text.strip() if rating else \"N/A\")\n",
    "\n",
    "    # Saving fetched data to DataFrame\n",
    "    data = {\n",
    "        \"Product Name\": product_names,\n",
    "        \"Price (₹)\": product_prices,\n",
    "        \"Rating\": product_ratings,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Saving the dataframe to a CSV file\n",
    "    df.to_csv(\"amazon_products.csv\", index=False)\n",
    "    print(\"Scraping completed. Data saved to amazon_products.csv\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64099d-f0c3-4f33-83f1-637f12e585d6",
   "metadata": {},
   "source": [
    "# Amazon Product Scraper\n",
    "\n",
    "This code scrapes product details such as names, prices, ratings, and URLs from Amazon and saves them into a CSV file. Below is the explanation of each part of the script:\n",
    "\n",
    "---\n",
    "\n",
    "## **Import Libraries**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "```\n",
    "1. **`requests`**: Used to send HTTP requests to fetch the HTML content of a webpage.\n",
    "2. **`BeautifulSoup`**: Parses the HTML content and provides methods for navigating and extracting data from it.\n",
    "3. **`pandas`**: Used to store the scraped data in a structured format (DataFrame) and save it to a CSV file.\n",
    "4. **`time`**: Introduces delays between requests to avoid being flagged as a bot.\n",
    "5. **`random`**: Generates random delays and rotates User-Agent headers to mimic real user behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## **Rotate User-Agent Headers**\n",
    "```python\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "]\n",
    "```\n",
    "- A list of different User-Agent strings simulates requests from different browsers and operating systems.\n",
    "- This helps avoid detection by Amazon's anti-scraping mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "## **Set Base URL**\n",
    "```python\n",
    "base_url = \"https://www.amazon.in/s?bbn=81107433031&rh=n%3A81107433031%2Cp_85%3A10440599031&page={page}\"\n",
    "```\n",
    "- The base URL is the template for Amazon's search results.\n",
    "- The `{page}` placeholder allows fetching multiple pages by substituting the page number.\n",
    "\n",
    "---\n",
    "\n",
    "## **Initialize Lists for Data**\n",
    "```python\n",
    "product_names, product_prices, product_ratings, product_urls = [], [], [], []\n",
    "```\n",
    "- These empty lists will store the scraped data:\n",
    "  - **`product_names`**: Names of the products.\n",
    "  - **`product_prices`**: Prices of the products.\n",
    "  - **`product_ratings`**: Ratings of the products.\n",
    "  - **`product_urls`**: URLs for the product detail pages.\n",
    "\n",
    "---\n",
    "\n",
    "## **Iterate Over Pages**\n",
    "```python\n",
    "for page in range(1, 4):  # Adjust range for more pages\n",
    "    print(f\"Scraping page {page}...\")\n",
    "```\n",
    "- Loops through the pages to scrape.\n",
    "- **`range(1, 4)`**: Specifies scraping from page 1 to page 3. Adjust the range for more pages.\n",
    "\n",
    "---\n",
    "\n",
    "## **Set Request Headers**\n",
    "```python\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "```\n",
    "- Rotates the User-Agent header to reduce the chances of being flagged as a bot.\n",
    "- Adds an \"Accept-Language\" header to indicate the preferred language for the response.\n",
    "\n",
    "---\n",
    "\n",
    "## **Send Request to Amazon**\n",
    "```python\n",
    "response = requests.get(base_url.format(page=page), headers=headers)\n",
    "```\n",
    "- Sends an HTTP GET request to fetch the HTML content of the current page.\n",
    "- **`base_url.format(page=page)`**: Replaces `{page}` with the current page number.\n",
    "\n",
    "---\n",
    "\n",
    "## **Handle Request Failure**\n",
    "```python\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "    continue\n",
    "```\n",
    "- Checks if the server responded successfully (status code `200`).\n",
    "- If the request fails, logs the error and skips to the next page.\n",
    "\n",
    "---\n",
    "\n",
    "## **Parse HTML Content**\n",
    "```python\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "```\n",
    "- Uses BeautifulSoup to parse the HTML content into a navigable format.\n",
    "\n",
    "---\n",
    "\n",
    "## **Find Product Container**\n",
    "```python\n",
    "product_container = soup.find(\"div\", {\"class\": \"s-main-slot s-result-list s-search-results sg-row\"})\n",
    "```\n",
    "- Finds the main container holding all the product listings using its class.\n",
    "- If this container is not found, the scraper skips further processing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Extract Product Details**\n",
    "```python\n",
    "for product in product_container.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "```\n",
    "- Iterates through all product elements within the container using their `data-component-type`.\n",
    "\n",
    "---\n",
    "\n",
    "### **a) Extract Product Name**\n",
    "```python\n",
    "name = product.find(\"span\", {\"class\": \"a-size-base-plus a-color-base a-text-normal\"})\n",
    "product_names.append(name.text.strip() if name else \"N/A\")\n",
    "```\n",
    "- Finds the product name using its class and appends it to `product_names`.\n",
    "- If the name is not found, appends `\"N/A\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### **b) Extract Product Price**\n",
    "```python\n",
    "price = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "product_prices.append(price.text.strip() if price else \"N/A\")\n",
    "```\n",
    "- Finds the product price using its class and appends it to `product_prices`.\n",
    "- If the price is not found, appends `\"N/A\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### **c) Extract Product Rating**\n",
    "```python\n",
    "rating = product.find(\"span\", {\"class\": \"a-icon-alt\"})\n",
    "product_ratings.append(rating.text.strip() if rating else \"N/A\")\n",
    "```\n",
    "- Finds the product rating using its class and appends it to `product_ratings`.\n",
    "- If the rating is not found, appends `\"N/A\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### **d) Extract Product URL**\n",
    "```python\n",
    "url_tag = product.find(\"a\", {\"class\": \"a-link-normal s-no-outline\"})\n",
    "product_url = f\"https://www.amazon.in{url_tag['href']}\" if url_tag else \"N/A\"\n",
    "product_urls.append(product_url)\n",
    "```\n",
    "- Finds the product detail page URL and appends it to `product_urls`.\n",
    "- Prepends the base URL (`https://www.amazon.in`) to make it a complete link.\n",
    "\n",
    "---\n",
    "\n",
    "## **Add Delay Between Requests**\n",
    "```python\n",
    "time.sleep(random.uniform(1, 3))\n",
    "```\n",
    "- Introduces a random delay (1-3 seconds) between requests to mimic human behavior and avoid being flagged as a bot.\n",
    "\n",
    "---\n",
    "\n",
    "## **Save Data to a DataFrame**\n",
    "```python\n",
    "df = pd.DataFrame({\n",
    "    \"Product Name\": product_names,\n",
    "    \"Price (₹)\": product_prices,\n",
    "    \"Rating\": product_ratings,\n",
    "    \"Product URL\": product_urls,\n",
    "})\n",
    "```\n",
    "- Creates a Pandas DataFrame to store the scraped data in a structured format.\n",
    "\n",
    "---\n",
    "\n",
    "## **Save Data to CSV**\n",
    "```python\n",
    "df.to_csv(\"amazon_products_detailed.csv\", index=False)\n",
    "print(\"Scraping completed. Data saved to amazon_products_detailed.csv\")\n",
    "```\n",
    "- Saves the DataFrame to a CSV file named `amazon_products_detailed.csv`.\n",
    "- Logs a message indicating successful completion.\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary**\n",
    "This script efficiently scrapes product data (name, price, rating, and URL) from Amazon using:\n",
    "1. **HTTP requests** with random User-Agent headers.\n",
    "2. **HTML parsing** using BeautifulSoup.\n",
    "3. **Data storage** with Pandas DataFrame and CSV.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fdf5214-5ad9-4243-941a-a25606dcafef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping completed. Data saved to amazon_products_detailed.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Rotating User-Agent headers\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "]\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.amazon.in/s?bbn=81107433031&rh=n%3A81107433031%2Cp_85%3A10440599031&page={page}\"\n",
    "\n",
    "# Initializing lists for data\n",
    "product_names, product_prices, product_ratings, product_urls = [], [], [], []\n",
    "\n",
    "# Scraping multiple pages\n",
    "for page in range(1, 4):  # Adjust range for more pages\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(user_agents),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "    response = requests.get(base_url.format(page=page), headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    product_container = soup.find(\"div\", {\"class\": \"s-main-slot s-result-list s-search-results sg-row\"})\n",
    "\n",
    "    if product_container:\n",
    "        for product in product_container.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "            # Extracting product name\n",
    "            name = product.find(\"span\", {\"class\": \"a-size-base-plus a-color-base a-text-normal\"})\n",
    "            # Extracting product price\n",
    "            price = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "            # Extracting product rating\n",
    "            rating = product.find(\"span\", {\"class\": \"a-icon-alt\"})\n",
    "            # Extracting product URL\n",
    "            url_tag = product.find(\"a\", {\"class\": \"a-link-normal s-no-outline\"})\n",
    "            product_url = f\"https://www.amazon.in{url_tag['href']}\" if url_tag else \"N/A\"\n",
    "\n",
    "            product_names.append(name.text.strip() if name else \"N/A\")\n",
    "            product_prices.append(price.text.strip() if price else \"N/A\")\n",
    "            product_ratings.append(rating.text.strip() if rating else \"N/A\")\n",
    "            product_urls.append(product_url)\n",
    "\n",
    "    # Adding delay to avoid being flagged as a bot\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# Saving fetched data to a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Product Name\": product_names,\n",
    "    \"Price (₹)\": product_prices,\n",
    "    \"Rating\": product_ratings,\n",
    "    \"Product URL\": product_urls,\n",
    "})\n",
    "\n",
    "# Saving dataframe to CSV\n",
    "df.to_csv(\"amazon_products_detailed.csv\", index=False)\n",
    "print(\"Scraping completed. Data saved to amazon_products_detailed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eba66f-db9a-4263-88be-45a815f574d4",
   "metadata": {},
   "source": [
    "#  Jumia Product Scraper\n",
    "\n",
    "This script scrapes product information (name, price, rating, and link) from the Jumia website, logs errors, and saves the data into a CSV file. Here's a detailed breakdown of the code:\n",
    "\n",
    "---\n",
    "\n",
    "## **Import Libraries**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "1. **`requests`**: Fetches HTML content of the webpage.\n",
    "2. **`BeautifulSoup`**: Parses and extracts data from the HTML.\n",
    "3. **`pandas`**: Stores scraped data in a structured DataFrame and saves it to a CSV file.\n",
    "4. **`random`**: Introduces randomness in delays and User-Agent headers to mimic real user behavior.\n",
    "5. **`time`**: Adds delays between requests to avoid detection as a bot.\n",
    "6. **`logging`**: Logs errors encountered during scraping to a file.\n",
    "7. **`matplotlib.pyplot`**: Prepares the script for data visualization (not used in scraping here).\n",
    "\n",
    "---\n",
    "\n",
    "## **Configure Logging**\n",
    "```python\n",
    "logging.basicConfig(filename=\"scraper.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "```\n",
    "- Configures logging to record errors in a file named `scraper.log`.\n",
    "- Logs include timestamps, error levels, and error messages for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## **User-Agent Rotation**\n",
    "```python\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "]\n",
    "```\n",
    "- A list of different User-Agent headers simulates requests from various browsers and operating systems, helping to avoid detection as a bot.\n",
    "\n",
    "---\n",
    "\n",
    "## **Base URL**\n",
    "```python\n",
    "base_url = \"https://www.jumia.co.ke/catalog/?q=smartphones&page={}\"\n",
    "```\n",
    "- Defines the base URL for the search results page. The `{}` acts as a placeholder for the page number.\n",
    "\n",
    "---\n",
    "\n",
    "## **Get Total Pages**\n",
    "```python\n",
    "def get_total_pages(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            pagination = soup.find(\"div\", class_=\"pg-w -ptm -pbxl\")\n",
    "            if pagination:\n",
    "                return int(pagination.find_all(\"a\")[-2].text.strip())  # Extract total pages\n",
    "        return 1  # Default to 1 if pagination not found\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching total pages: {e}\")\n",
    "        return 1\n",
    "```\n",
    "1. **Purpose**: Determines the total number of pages in the search results.\n",
    "2. **Process**:\n",
    "   - Sends a request to the first page.\n",
    "   - Parses the pagination element to find the total number of pages.\n",
    "3. **Error Handling**:\n",
    "   - Logs errors in fetching total pages and defaults to 1 page.\n",
    "\n",
    "---\n",
    "\n",
    "## **Scrape Product Data**\n",
    "```python\n",
    "def scrape_jumia(num_pages):\n",
    "    product_data = []\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(user_agents),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "```\n",
    "- Initializes an empty list (`product_data`) to store scraped data.\n",
    "- Sets headers with a random User-Agent for each request.\n",
    "\n",
    "---\n",
    "\n",
    "### **Iterate Over Pages**\n",
    "```python\n",
    "for page in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    try:\n",
    "        response = requests.get(base_url.format(page), headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "            continue\n",
    "```\n",
    "- Iterates over the total number of pages.\n",
    "- Sends a GET request to each page using the formatted URL.\n",
    "- Logs an error if the request fails (non-200 status code).\n",
    "\n",
    "---\n",
    "\n",
    "### **Parse HTML Content**\n",
    "```python\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "products = soup.find_all(\"article\", class_=\"prd _fb col c-prd\")\n",
    "```\n",
    "- Parses the HTML response using BeautifulSoup.\n",
    "- Finds all product containers with the specified class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Extract Product Details**\n",
    "```python\n",
    "for product in products:\n",
    "    # Extract product name\n",
    "    name_tag = product.find(\"h3\", class_=\"name\")\n",
    "    name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "\n",
    "    # Extract product price\n",
    "    price_tag = product.find(\"div\", class_=\"prc\")\n",
    "    price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "    # Extract product link\n",
    "    link_tag = product.find(\"a\", href=True)\n",
    "    link = \"https://www.jumia.co.ke\" + link_tag[\"href\"] if link_tag else \"N/A\"\n",
    "\n",
    "    # Extract product rating\n",
    "    rating_tag = product.find(\"div\", class_=\"stars\")\n",
    "    rating = rating_tag.get(\"aria-label\") if rating_tag else \"N/A\"\n",
    "\n",
    "    product_data.append({\n",
    "        \"Product Name\": name,\n",
    "        \"Price\": price,\n",
    "        \"Rating\": rating,\n",
    "        \"Product Link\": link\n",
    "    })\n",
    "```\n",
    "1. **`name`**: Extracts the product name from the `h3` tag.\n",
    "2. **`price`**: Extracts the product price from the `div` tag.\n",
    "3. **`link`**: Extracts the product detail URL and appends the base URL to it.\n",
    "4. **`rating`**: Extracts the rating using the `aria-label` attribute.\n",
    "5. Appends the extracted details as a dictionary to the `product_data` list.\n",
    "\n",
    "---\n",
    "\n",
    "### **Add Delay Between Requests**\n",
    "```python\n",
    "time.sleep(random.uniform(1, 3))\n",
    "```\n",
    "- Introduces a random delay (1-3 seconds) between requests to avoid being flagged as a bot.\n",
    "\n",
    "---\n",
    "\n",
    "## **Save Data to CSV**\n",
    "```python\n",
    "df = pd.DataFrame(product_data)\n",
    "df.to_csv(\"jumia_enhanced_products.csv\", index=False)\n",
    "print(\"Scraping completed. Data saved to jumia_enhanced_products.csv\")\n",
    "```\n",
    "1. Creates a Pandas DataFrame from the `product_data` list.\n",
    "2. Saves the DataFrame to a CSV file named `jumia_enhanced_products.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Script Execution**\n",
    "```python\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "total_pages = get_total_pages(base_url.format(1), headers)\n",
    "print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "df = scrape_jumia(total_pages)\n",
    "```\n",
    "1. Calls the `get_total_pages` function to determine the number of pages.\n",
    "2. Calls the `scrape_jumia` function to scrape data from all pages.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "This script:\n",
    "- Scrapes product names, prices, ratings, and links from Jumia.\n",
    "- Handles errors gracefully and logs them for debugging.\n",
    "- Saves the scraped data into a CSV file for further use.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "344361d2-24e2-478f-8c3e-b2326bf3d0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"scraper.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Rotating User-Agent headers to mimic browser behavior\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "]\n",
    "\n",
    "# Base URL for Jumia\n",
    "base_url = \"https://www.jumia.co.ke/catalog/?q=smartphones&page={}\"\n",
    "\n",
    "# Function to get total number of pages\n",
    "def get_total_pages(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            pagination = soup.find(\"div\", class_=\"pg-w -ptm -pbxl\")\n",
    "            if pagination:\n",
    "                return int(pagination.find_all(\"a\")[-2].text.strip())  # Extracting total pages\n",
    "        return 1  # Default to 1 if pagination not found\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching total pages: {e}\")\n",
    "        return 1\n",
    "\n",
    "# Function to scrape product data\n",
    "def scrape_jumia(num_pages):\n",
    "    product_data = []\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(user_agents),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        try:\n",
    "            response = requests.get(base_url.format(page), headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                logging.error(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            products = soup.find_all(\"article\", class_=\"prd _fb col c-prd\")  # Target product containers\n",
    "            \n",
    "            for product in products:\n",
    "                # Extracting product name\n",
    "                name_tag = product.find(\"h3\", class_=\"name\")\n",
    "                name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "\n",
    "                # Extracting product price\n",
    "                price_tag = product.find(\"div\", class_=\"prc\")\n",
    "                price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "                # Extracting product link\n",
    "                link_tag = product.find(\"a\", href=True)\n",
    "                link = \"https://www.jumia.co.ke\" + link_tag[\"href\"] if link_tag else \"N/A\"\n",
    "\n",
    "                # Extracting product rating\n",
    "                rating_tag = product.find(\"div\", class_=\"stars\")\n",
    "                rating = rating_tag.get(\"aria-label\") if rating_tag else \"N/A\"\n",
    "\n",
    "                product_data.append({\n",
    "                    \"Product Name\": name,\n",
    "                    \"Price\": price,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Product Link\": link\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error on page {page}: {e}\")\n",
    "\n",
    "        # Random delay to mimic human behavior\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    return pd.DataFrame(product_data)\n",
    "# Scraping data\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "total_pages = get_total_pages(base_url.format(1), headers)\n",
    "print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "# Scraping all pages\n",
    "df = scrape_jumia(total_pages)\n",
    "df.to_csv(\"jumia_enhanced_products.csv\", index=False)\n",
    "print(\"Scraping completed. Data saved to jumia_enhanced_products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26c004-11d5-4505-9c1c-db2111971f3b",
   "metadata": {},
   "source": [
    "# Jumia Product Scraper with Ratings\n",
    "\n",
    "This script scrapes product information, including names, prices, ratings, and links, from Jumia's website, saves the data into a CSV file, and logs any errors during the process. Here's a detailed breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Import Libraries**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import logging\n",
    "```\n",
    "- **`requests`**: Handles HTTP requests to fetch webpage content.\n",
    "- **`BeautifulSoup`**: Parses and extracts data from HTML content.\n",
    "- **`pandas`**: Creates a DataFrame and saves data to a CSV file.\n",
    "- **`selenium - webdriver`**: Mimick web browsing activity.\n",
    "- **`logging`**: Logs errors and debugging information to a file.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Configure Logging**\n",
    "```python\n",
    "logging.basicConfig(filename=\"scraper.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "```\n",
    "- Configures logging to write errors into a file named `scraper.log`.\n",
    "- Logs include:\n",
    "  - **Timestamp** (`%(asctime)s`)\n",
    "  - **Error Level** (`%(levelname)s`)\n",
    "  - **Error Message** (`%(message)s`).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Define Base URL**\n",
    "```python\n",
    "base_url = \"https://www.jumia.co.ke/catalog/?q=smartphones&page={}\"\n",
    "```\n",
    "- The base URL contains a query for \"smartphones\" with pagination indicated by `{}`.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Get Total Pages**\n",
    "```python\n",
    "def get_total_pages(url):\n",
    "    try:\n",
    "        response = webdriver.Chrome()\n",
    "        response.get(url)\n",
    "        soup = BeautifulSoup(response.page_source, \"html.parser\")\n",
    "        pagination = soup.find(\"div\", class_=\"pg-w -ptm -pbxl\")\n",
    "        response.close()\n",
    "        if pagination:\n",
    "          return int(''.join(filter(str.isdigit, pagination.find_all(\"a\")[-1].get(\"href\")))) # Extracting total pages\n",
    "        return 1  # Default to 1 if pagination not found\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching total pages: {e}\")\n",
    "        return 1\n",
    "```\n",
    "- **Purpose**: Extracts the total number of pages from the pagination element.\n",
    "- **Logic**:\n",
    "  - Sends a request to the first page.\n",
    "  - Finds the pagination container and extracts the second-to-last page number.\n",
    "  - Returns 1 if no pagination is found.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Scrape Product Data**\n",
    "```python\n",
    "def scrape_jumia(num_pages):\n",
    "    product_data = []\n",
    "```\n",
    "- Initializes an empty list, `product_data`, to store product information.\n",
    "- Randomizes the User-Agent for each request.\n",
    "\n",
    "---\n",
    "\n",
    "### **a) Iterate Over Pages**\n",
    "```python\n",
    "for page in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    try:\n",
    "        response = webdriver.Chrome()\n",
    "        response.get(base_url.format(page))\n",
    "```\n",
    "- Loops through all pages.\n",
    "\n",
    "---\n",
    "\n",
    "### **b) Parse HTML Content**\n",
    "```python\n",
    "soup = BeautifulSoup(response.page_source, \"html.parser\")\n",
    "products = soup.find_all(\"article\", class_=\"prd _fb col c-prd\")\n",
    "```\n",
    "- Parses the HTML response using BeautifulSoup.\n",
    "- Finds all product containers matching the specified class.\n",
    "\n",
    "---\n",
    "\n",
    "### **c) Extract Product Details**\n",
    "```python\n",
    "for product in products:\n",
    "    # Extract product name\n",
    "    name_tag = product.find(\"h3\", class_=\"name\")\n",
    "    name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "\n",
    "    # Extract product price\n",
    "    price_tag = product.find(\"div\", class_=\"prc\")\n",
    "    price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "    # Extract product link\n",
    "    link_tag = product.find(\"a\", href=True)\n",
    "    link = \"https://www.jumia.co.ke\" + link_tag[\"href\"] if link_tag else \"N/A\"\n",
    "\n",
    "    # Extract product rating\n",
    "    rating_tag = product.find(\"div\", class_=\"stars _s\")\n",
    "    if rating_tag:\n",
    "        rating = rating_tag.text.strip()  # Extract visible rating\n",
    "    else:\n",
    "        rating = \"N/A\"\n",
    "\n",
    "    product_data.append({\n",
    "        \"Product Name\": name,\n",
    "        \"Price\": price,\n",
    "        \"Rating\": rating,\n",
    "        \"Product Link\": link\n",
    "    })\n",
    "```\n",
    "1. **`name`**: Extracts the product name from the `h3` tag.\n",
    "2. **`price`**: Extracts the product price from the `div` tag.\n",
    "3. **`link`**: Constructs the full product URL.\n",
    "4. **`rating`**: Extracts the rating (if available).\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Save Data to CSV**\n",
    "```python\n",
    "df = pd.DataFrame(product_data)\n",
    "df.to_csv(\"jumia_enhanced_with_ratings.csv\", index=False)\n",
    "print(\"Scraping completed. Data saved to jumia_enhanced_with_ratings.csv\")\n",
    "```\n",
    "1. Creates a Pandas DataFrame from the `product_data` list.\n",
    "2. Saves the DataFrame to a CSV file named `jumia_enhanced_with_ratings.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Main Script Execution**\n",
    "```python\n",
    "total_pages = get_total_pages(base_url.format(1), headers)\n",
    "print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "df = scrape_jumia(total_pages)\n",
    "```\n",
    "1. Calls `get_total_pages` to determine the number of pages to scrape.\n",
    "2. Calls `scrape_jumia` to extract product data from all pages.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "1. Scrapes product names, prices, ratings, and URLs from Jumia.\n",
    "2. Handles errors gracefully with logging.\n",
    "3. Saves the scraped data into a CSV file.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"scraper.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "# Base URL for Jumia\n",
    "base_url = \"https://www.jumia.co.ke/catalog/?q=smartphones&page={}\"\n",
    "\n",
    "# Function to get total number of pages\n",
    "def get_total_pages(url):\n",
    "    try:\n",
    "        response = webdriver.Chrome()\n",
    "        response.get(url)\n",
    "        soup = BeautifulSoup(response.page_source, \"html.parser\")\n",
    "        pagination = soup.find(\"div\", class_=\"pg-w -ptm -pbxl\")\n",
    "        response.close()\n",
    "        if pagination:\n",
    "          return int(''.join(filter(str.isdigit, pagination.find_all(\"a\")[-1].get(\"href\")))) # Extracting total pages\n",
    "        return 1  # Default to 1 if pagination not found\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching total pages: {e}\")\n",
    "        return 1\n",
    "\n",
    "# Function to scrape product data\n",
    "def scrape_jumia(num_pages):\n",
    "    product_data = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        try:\n",
    "            response = webdriver.Chrome()\n",
    "            response.get(base_url.format(page))\n",
    "\n",
    "            soup = BeautifulSoup(response.page_source, \"html.parser\")\n",
    "            products = soup.find_all(\"article\", class_=\"prd _fb col c-prd\")  # Target product containers\n",
    "            \n",
    "            for product in products:\n",
    "                # Extracting product name\n",
    "                name_tag = product.find(\"h3\", class_=\"name\")\n",
    "                name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "\n",
    "                # Extracting product price\n",
    "                price_tag = product.find(\"div\", class_=\"prc\")\n",
    "                price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "                # Extracting product link\n",
    "                link_tag = product.find(\"a\", href=True)\n",
    "                link = \"https://www.jumia.co.ke\" + link_tag[\"href\"] if link_tag else \"N/A\"\n",
    "\n",
    "                # Extract product rating\n",
    "                rating_tag = product.find(\"div\", class_=\"stars _s\")\n",
    "                if rating_tag:\n",
    "                    rating = rating_tag.text.strip()  # Extracting visible rating\n",
    "                else:\n",
    "                    rating = \"N/A\"\n",
    "\n",
    "                product_data.append({\n",
    "                    \"Product Name\": name,\n",
    "                    \"Price\": price,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Product Link\": link\n",
    "                })\n",
    "            response.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error on page {page}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(product_data)\n",
    "\n",
    "total_pages = get_total_pages(base_url.format(1))\n",
    "print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "# Scraping all pages\n",
    "df = scrape_jumia(total_pages)\n",
    "df.to_csv(\"jumia_enhanced_with_ratings.csv\", index=False)\n",
    "print(\"Scraping completed. Data saved to jumia_enhanced_with_ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392fac2-12d4-4935-b210-df2f6931b0f9",
   "metadata": {},
   "source": [
    "                                                  Thank You !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
